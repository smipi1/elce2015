
kmalloc accounting support for slob



 mm/slob.c |   55 	45 +	10 -	0 !
 1 files changed, 45 insertions(+), 10 deletions(-)

Index: linux-2.6.23-rc9-tiny/mm/slob.c
===================================================================
--- linux-2.6.23-rc9-tiny.orig/mm/slob.c	2007-10-05 14:23:34.000000000 +0200
+++ linux-2.6.23-rc9-tiny/mm/slob.c	2007-10-08 22:52:37.000000000 +0200
@@ -155,6 +155,9 @@
 #define SLOB_UNIT sizeof(slob_t)
 #define SLOB_UNITS(size) (((size) + SLOB_UNIT - 1)/SLOB_UNIT)
 #define SLOB_ALIGN L1_CACHE_BYTES
+#define SLOB_ACCT(size) ALIGN(size, SLOB_UNIT)
+#define SLOB_ACCT_S(obj) SLOB_ACCT(sizeof(obj))
+#define PAGE_ACCT(order) (PAGE_SIZE<<order)
 
 /*
  * struct slob_rcu is inserted at the tail of allocated slob blocks, which
@@ -166,6 +169,17 @@
 	int size;
 };
 
+#ifdef CONFIG_DEBUG_SLOB_ACCOUNT
+int slob_total, slob_req;
+
+static void slob_acct(int real, int req) {
+	slob_total += real;
+	slob_req += req;
+}
+#else
+#define slob_acct(a, b)
+#endif
+
 /*
  * slob_lock protects all slob allocator structures.
  */
@@ -534,6 +548,7 @@
 void kmem_cache_destroy(struct kmem_cache *c)
 {
 	slob_free(c, sizeof(struct kmem_cache));
+	cache_free_account(c, (int) SLOT_ACCT_S(kmem_cache_t));
 }
 EXPORT_SYMBOL(kmem_cache_destroy);
 
@@ -555,10 +570,13 @@
 
 static void __kmem_cache_free(void *b, int size)
 {
-	if (size < PAGE_SIZE)
-		slob_free(b, size);
-	else
-		free_pages((unsigned long)b, get_order(size));
+ 	if (size < PAGE_SIZE) {
+ 		slob_free(b, size);
+ 		cache_free_account(b, SLOB_ACCT(size));
+ 	} else {
+ 		free_pages((unsigned long)b, get_order(size));
+ 		cache_free_account(b, PAGE_ACCT(get_order(size)));
+ 	}
 }
 
 static void kmem_rcu_free(struct rcu_head *head)
