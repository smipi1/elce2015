
/proc/slab_account (cache allocation) tracing


 fs/proc/proc_misc.c  |   21 	21 +	0 -	0 !
 include/linux/slab.h |   18 	18 +	0 -	0 !
 lib/Kconfig.debug    |    8 	8 +	0 -	0 !
 mm/Makefile          |    1 	1 +	0 -	0 !
 mm/slab.c            |   33 	21 +	12 -	0 !
 mm/slabacct.c        |  247 	247 +	0 -	0 !
 6 files changed, 316 insertions(+), 12 deletions(-)

Index: linux-2.6.23-rc9/fs/proc/proc_misc.c
===================================================================
--- linux-2.6.23-rc9.orig/fs/proc/proc_misc.c	2007-10-08 23:51:21.000000000 +0200
+++ linux-2.6.23-rc9/fs/proc/proc_misc.c	2007-10-09 00:30:05.000000000 +0200
@@ -438,6 +438,24 @@
 #endif
 #endif
 
+#ifdef CONFIG_DEBUG_SLAB_ACCOUNT
+
+extern struct seq_operations slab_account_op;
+
+static int slab_account_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &slab_account_op);
+}
+
+static struct file_operations proc_slab_account_operations = {
+	.open		= slab_account_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= seq_release,
+};
+
+#endif
+
 static int show_stat(struct seq_file *p, void *v)
 {
 	int i;
@@ -715,6 +733,9 @@
 	create_seq_entry("slab_allocators", 0 ,&proc_slabstats_operations);
 #endif
 #endif
+#ifdef CONFIG_DEBUG_SLAB_ACCOUNT
+	create_seq_entry("slab_account",S_IRUGO,&proc_slab_account_operations);
+#endif
 	create_seq_entry("buddyinfo",S_IRUGO, &fragmentation_file_operations);
 	create_seq_entry("vmstat",S_IRUGO, &proc_vmstat_file_operations);
 	create_seq_entry("zoneinfo",S_IRUGO, &proc_zoneinfo_file_operations);
Index: linux-2.6.23-rc9/include/linux/slab.h
===================================================================
--- linux-2.6.23-rc9.orig/include/linux/slab.h	2007-10-08 23:51:25.000000000 +0200
+++ linux-2.6.23-rc9/include/linux/slab.h	2007-10-08 23:52:27.000000000 +0200
@@ -49,6 +49,24 @@
 void __init kmem_cache_init(void);
 int slab_is_available(void);
 
+#ifdef CONFIG_DEBUG_SLAB_ACCOUNT
+void __cache_alloc_account(const void *, const void *, int, int);
+void __cache_free_account(const void *, int);
+
+static void inline cache_alloc_account(void *caller, const void *addr, int size, int req)
+{
+	__cache_alloc_account(caller, addr, size, req);
+}
+
+static void inline cache_free_account(const void *addr, int size)
+{
+	__cache_free_account(addr, size);
+}
+#else
+#define cache_alloc_account(a, b, c, d)
+#define cache_free_account(a, b)
+#endif
+
 struct kmem_cache *kmem_cache_create(const char *, size_t, size_t,
 			unsigned long,
 			void (*)(void *, struct kmem_cache *, unsigned long));
Index: linux-2.6.23-rc9/mm/Makefile
===================================================================
--- linux-2.6.23-rc9.orig/mm/Makefile	2007-10-08 23:51:25.000000000 +0200
+++ linux-2.6.23-rc9/mm/Makefile	2007-10-08 23:52:27.000000000 +0200
@@ -14,6 +14,7 @@
 			   $(mmu-y)
 
 obj-$(CONFIG_BOUNCE)	+= bounce.o
+obj-$(CONFIG_DEBUG_SLAB_ACCOUNT) += slabacct.o
 obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o thrash.o
 obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o
 obj-$(CONFIG_NUMA) 	+= mempolicy.o
Index: linux-2.6.23-rc9/mm/slab.c
===================================================================
--- linux-2.6.23-rc9.orig/mm/slab.c	2007-10-08 23:51:25.000000000 +0200
+++ linux-2.6.23-rc9/mm/slab.c	2007-10-08 23:52:27.000000000 +0200
@@ -2858,6 +2858,8 @@
 	unsigned int objnr;
 	struct slab *slabp;
 
+	cache_free_account(objp, cachep->obj_size);
+
 	objp -= obj_offset(cachep);
 	kfree_debugcheck(objp);
 	page = virt_to_head_page(objp);
@@ -3032,7 +3034,8 @@
 
 #if DEBUG
 static void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,
-				gfp_t flags, void *objp, void *caller)
+				gfp_t flags, void *objp, void *caller,
+				int req_size)
 {
 	if (!objp)
 		return objp;
@@ -3083,10 +3086,11 @@
 		       objp, ARCH_SLAB_MINALIGN);
 	}
 #endif
+	cache_alloc_account(caller, objp, cachep->obj_size, req_size);
 	return objp;
 }
 #else
-#define cache_alloc_debugcheck_after(a,b,objp,d) (objp)
+#define cache_alloc_debugcheck_after(a,b,objp,d,e) (objp)
 #endif
 
 #ifdef CONFIG_FAILSLAB
@@ -3355,7 +3359,7 @@
  */
 static __always_inline void *
 __cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,
-		   void *caller)
+		   void *caller, int size)
 {
 	unsigned long save_flags;
 	void *ptr;
@@ -3390,7 +3394,7 @@
 	ptr = ____cache_alloc_node(cachep, flags, nodeid);
   out:
 	local_irq_restore(save_flags);
-	ptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);
+	ptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller, size);
 
 	if (unlikely((flags & __GFP_ZERO) && ptr))
 		memset(ptr, 0, obj_size(cachep));
@@ -3431,7 +3435,8 @@
 #endif /* CONFIG_NUMA */
 
 static __always_inline void *
-__cache_alloc(struct kmem_cache *cachep, gfp_t flags, void *caller)
+__cache_alloc(struct kmem_cache *cachep, gfp_t flags, void *caller,
+		size_t size)
 {
 	unsigned long save_flags;
 	void *objp;
@@ -3443,7 +3448,7 @@
 	local_irq_save(save_flags);
 	objp = __do_cache_alloc(cachep, flags);
 	local_irq_restore(save_flags);
-	objp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);
+	objp = cache_alloc_debugcheck_after(cachep, flags, objp, caller, size);
 	prefetchw(objp);
 
 	if (unlikely((flags & __GFP_ZERO) && objp))
@@ -3592,7 +3597,7 @@
  */
 void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
 {
-	return __cache_alloc(cachep, flags, __builtin_return_address(0));
+	return __cache_alloc(cachep, flags, __builtin_return_address(0), -1);
 }
 EXPORT_SYMBOL(kmem_cache_alloc);
 
@@ -3642,7 +3647,7 @@
 void *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)
 {
 	return __cache_alloc_node(cachep, flags, nodeid,
-			__builtin_return_address(0));
+			__builtin_return_address(0), -1);
 }
 EXPORT_SYMBOL(kmem_cache_alloc_node);
 
@@ -3654,7 +3659,7 @@
 	cachep = kmem_find_general_cachep(size, flags);
 	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
 		return cachep;
-	return kmem_cache_alloc_node(cachep, flags, node);
+	return kmem_cache_alloc_node(cachep, flags, node, size);
 }
 
 #ifdef CONFIG_DEBUG_SLAB
@@ -3699,7 +3704,7 @@
 	cachep = __find_general_cachep(size, flags);
 	if (unlikely(ZERO_OR_NULL_PTR(cachep)))
 		return cachep;
-	return __cache_alloc(cachep, flags, caller);
+	return __cache_alloc(cachep, flags, caller, size);
 }
 
 
@@ -3759,8 +3764,11 @@
 	struct kmem_cache *c;
 	unsigned long flags;
 
-	if (unlikely(ZERO_OR_NULL_PTR(objp)))
-		return;
+ 	if (unlikely(ZERO_OR_NULL_PTR(objp))) {
+ 		cache_free_account(objp, 0);
+ 		return;
+ 	}
+
 	local_irq_save(flags);
 	kfree_debugcheck(objp);
 	c = virt_to_cache(objp);
Index: linux-2.6.23-rc9/lib/Kconfig.debug
===================================================================
--- linux-2.6.23-rc9.orig/lib/Kconfig.debug	2007-10-08 23:51:25.000000000 +0200
+++ linux-2.6.23-rc9/lib/Kconfig.debug	2007-10-08 23:52:27.000000000 +0200
@@ -165,6 +165,14 @@
 	  off in a kernel built with CONFIG_SLUB_DEBUG_ON by specifying
 	  "slub_debug=-".
 
+config DEBUG_SLAB_ACCOUNT
+	default n
+	bool "Enabled accounting of slab allocations"
+	depends on DEBUG_SLAB
+	help
+	  This option records slab cache activity and reports it via
+	  /proc/slabaccount.
+
 config DEBUG_PREEMPT
 	bool "Debug preemptible kernel"
 	depends on DEBUG_KERNEL && PREEMPT && TRACE_IRQFLAGS_SUPPORT
Index: linux-2.6.23-rc9/mm/slabacct.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.23-rc9/mm/slabacct.c	2007-10-08 23:52:27.000000000 +0200
@@ -0,0 +1,247 @@
+/*
+ * linux/mm/slabacct.c - provide a method to track callers to slab APIs
+ *
+ * This used to be called kmalloc_acct.c, and only tracked callers
+ * to kmalloc.  There are lots of residual references to this previous
+ * name (e.g. kma_caller)
+ *
+ * Written originally by Matt Mackall
+ *
+ * 15 August 2007 - Switch to track all of slab allocators - Tim Bird
+ *     Change some names from k* to cache_*.  Also, fix some bugs
+ *     with free(0).  Introduce highwater, dup_frees and zero_frees stats.
+ */
+#include	<linux/spinlock.h>
+#include	<linux/seq_file.h>
+#include	<linux/kallsyms.h>
+
+
+struct kma_caller {
+	const void *caller;
+	int total, net, slack, allocs, frees, highwater;
+};
+
+struct kma_list {
+	int callerhash;
+	const void *address;
+};
+
+#define MAX_CALLER_TABLE 512
+/* #define MAX_ALLOC_TRACK 4096 */
+#define MAX_ALLOC_TRACK 8192
+
+#define kma_hash(address, size) (((u32)address / (u32)size) % size)
+
+static struct kma_list kma_alloc[MAX_ALLOC_TRACK];
+static struct kma_caller kma_caller[MAX_CALLER_TABLE];
+
+static int kma_callers;
+static int kma_lost_callers, kma_lost_allocs, kma_unknown_frees;
+static int kma_total, kma_net, kma_slack, kma_allocs, kma_frees;
+static int kma_highwater, kma_dup_frees, kma_zero_frees;
+static spinlock_t kma_lock = SPIN_LOCK_UNLOCKED;
+
+void __cache_alloc_account(const void *caller, const void *addr, int size, int req)
+{
+	int i, hasha, hashc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kma_lock, flags);
+
+	if (req == -1) {
+		req = size;
+	}
+
+	/* find callers slot */
+	hashc = kma_hash(caller, MAX_CALLER_TABLE);
+	/* don't allow use of the 0th hash slot */
+	if (hashc==0) {
+		hashc++;
+	}
+	for (i = 0; i < MAX_CALLER_TABLE; i++) {
+		if (!kma_caller[hashc].caller ||
+		    kma_caller[hashc].caller == caller)
+			break;
+		hashc = (hashc + 1) % MAX_CALLER_TABLE;
+		if (hashc == 0) {
+			hashc++;
+		}
+	}
+
+	if (!kma_caller[hashc].caller)
+		kma_callers++;
+
+	if (i < MAX_CALLER_TABLE) {
+		/* update callers stats */
+		kma_caller[hashc].caller = caller;
+		kma_caller[hashc].total += size;
+		kma_caller[hashc].net += size;
+		if ( kma_caller[hashc].net > kma_caller[hashc].highwater ) {
+			kma_caller[hashc].highwater = kma_caller[hashc].net;
+		}
+		kma_caller[hashc].slack += size - req;
+		kma_caller[hashc].allocs++;
+
+		/* add malloc to list */
+		hasha = kma_hash(addr, MAX_ALLOC_TRACK);
+		for (i = 0; i < MAX_ALLOC_TRACK; i++) {
+			if (kma_alloc[hasha].callerhash == 0 ||
+				kma_alloc[hasha].callerhash == -1)
+				break;
+			hasha = (hasha + 1) % MAX_ALLOC_TRACK;
+		}
+
+		if(i < MAX_ALLOC_TRACK) {
+			kma_alloc[hasha].callerhash = hashc;
+			kma_alloc[hasha].address = addr;
+		}
+		else
+			kma_lost_allocs++;
+	}
+	else {
+		kma_lost_callers++;
+		kma_lost_allocs++;
+	}
+
+	kma_total += size;
+	kma_net += size;
+	if (kma_net > kma_highwater ) {
+		kma_highwater = kma_net;
+	}
+	kma_slack += size - req;
+	kma_allocs++;
+
+	spin_unlock_irqrestore(&kma_lock, flags);
+}
+
+void __cache_free_account(const void *addr, int size)
+{
+	int i, hasha, hashc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kma_lock, flags);
+
+	kma_frees++;
+
+	if (unlikely(addr==0)) {
+		kma_zero_frees++;
+		goto kfree_unlock_exit;
+	}
+
+	kma_net -= size;
+
+	/* find allocation record */
+	hasha = kma_hash(addr, MAX_ALLOC_TRACK);
+	for (i = 0; i < MAX_ALLOC_TRACK ; i++) {
+		if (kma_alloc[hasha].address == addr)
+			break;
+		hasha = (hasha + 1) % MAX_ALLOC_TRACK;
+	}
+
+	if (i < MAX_ALLOC_TRACK) {
+		hashc = kma_alloc[hasha].callerhash;
+		if (hashc != -1) {
+			kma_alloc[hasha].callerhash = -1;
+			kma_caller[hashc].net -= size;
+			kma_caller[hashc].frees++;
+		} else {
+			/* found address in hash table, but it was
+			 * listed as already freed.
+			 */
+			kma_dup_frees++;
+		}
+	} else {
+		/* couldn't find address in hash table */
+		kma_unknown_frees++;
+	}
+
+kfree_unlock_exit:
+	spin_unlock_irqrestore(&kma_lock, flags);
+}
+
+
+static void *as_start(struct seq_file *m, loff_t *pos)
+{
+	int i;
+	loff_t n = *pos;
+
+	if (!n) {
+		seq_printf(m, "total bytes allocated: %8d\n", kma_total);
+		seq_printf(m, "slack bytes allocated: %8d\n", kma_slack);
+		seq_printf(m, "net bytes allocated:   %8d\n", kma_net);
+		seq_printf(m, "net highwater:         %8d\n", kma_highwater);
+		seq_printf(m, "number of allocs:      %8d\n", kma_allocs);
+		seq_printf(m, "number of frees:       %8d\n", kma_frees);
+		seq_printf(m, "number of callers:     %8d\n", kma_callers);
+		seq_printf(m, "lost callers:          %8d\n",
+			   kma_lost_callers);
+		seq_printf(m, "lost allocs:           %8d\n",
+			   kma_lost_allocs);
+		seq_printf(m, "unknown frees:         %8d\n",
+			   kma_unknown_frees);
+		seq_printf(m, "duplicate frees:       %8d\n", kma_dup_frees);
+		seq_printf(m, "zero frees:            %8d\n", kma_zero_frees);
+		seq_puts(m, "\n   total    slack      net alloc/free highwater caller\n");
+	}
+
+	for (i = 0; i < MAX_CALLER_TABLE; i++) {
+		if(kma_caller[i].caller)
+			n--;
+		if(n < 0)
+			return (void *)(i+1);
+	}
+
+	return 0;
+}
+
+static void *as_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	int n = (int)p-1, i;
+	++*pos;
+
+	for (i = n + 1; i < MAX_CALLER_TABLE; i++)
+		if(kma_caller[i].caller)
+			return (void *)(i+1);
+
+	return 0;
+}
+
+static void as_stop(struct seq_file *m, void *p)
+{
+}
+
+static int as_show(struct seq_file *m, void *p)
+{
+	int n = (int)p-1;
+	struct kma_caller *c;
+#ifdef CONFIG_KALLSYMS
+	char *modname;
+	const char *name;
+	unsigned long offset = 0, size;
+	char namebuf[128];
+
+	c = &kma_caller[n];
+	name = kallsyms_lookup((int)c->caller, &size, &offset, &modname,
+			       namebuf);
+	seq_printf(m, "%8d %8d %8d %5d/%-5d %8d %s+0x%lx\n",
+		   c->total, c->slack, c->net, c->allocs, c->frees,
+		   c->highwater,
+		   name, offset);
+#else
+	c = &kma_caller[n];
+	seq_printf(m, "%8d %8d %8d %5d/%-5d %8d %p\n",
+		   c->total, c->slack, c->net, c->allocs, c->frees,
+		   c->highwater,
+		   c->caller);
+#endif
+
+	return 0;
+}
+
+struct seq_operations slab_account_op = {
+	.start	= as_start,
+	.next	= as_next,
+	.stop	= as_stop,
+	.show	= as_show,
+};
+
